{
  "thesis_text": "AI Security Compliance and Risk  \nAssessment Framework for Large \nLanguage Model Systems  \n   \nSaniya Bhaladhare  \n   \nA Master\u2019s Thesis Proposal submitted \nin partial fulfillment of the \nrequirements of the degree of  \n   \nMaster of Science in Cybersecurity Engineering  \n   \nUniversity of Washington  \n7th October 2025  \n   \nProject Committee:  \n\u25cf Dr.  Min Chen, Committee Chair \n\u25cf Dr. Yang Peng, Committee Member \n\u25cf Dr. Marc Dupuis, Committee \nMember \n  \n  \n  \n  \n   \n I.  Introduction  \nThe rapid adoption of Artificial Intelligence, particularly Large Language Models, has transformed \nhow organizations operate, automate, and deliver value. From internal knowledge management \nto customer service automation, LLMs are now embedded in critical business workflows across \nindustries. However, this widespread integration has outpaced the maturity of compliance and \nsecurity practices. Organizations are increasingly confronted with the limitations of broad \nregulatory frameworks such as the NIST AI Risk Management Framework (AI RMF) and ISO/IEC \n42001, which offer essential principles but lack actionable, AI -specific guidance for real -world \nimplementation. As a result, compliance efforts often rely on manual audits that demand \nspecialized expertise, significant time investment, and stil l yield inconsistent results across \ncontexts.  \n This thesis investigates whether interactive, LLM-driven dialogue systems can serve as a viable \nmethod for conducting AI compliance and risk assessments. The research aims to model how \norganizational context can be captured through structured LLM interact ions, how applicable \nregulatory controls can be dynamically identified, and how tailored audit paths can be generated \nin alignment with recognized standards. It further evaluates whether such a framework can \nproduce traceable, control-level findings and strategic recommendations that are both actionable \nand scalable. By grounding the work in regulatory theory and empirical validation, this research \ncontributes toward a more adaptive and automated compliance methodology for modern AI \ndeployments.  \nII.  Project Goals / Vision  \n     A.     Goals of Completed Work  \nThe overarching goal of this thesis is to design and evaluate an AI -powered framework that can \nautomate and contextualize security compliance assessments for Large Language Model \nsystems. The research investigates whether interactive LLM -based dialogue sys tems can be \nused not only to guide organizational stakeholders through complex regulatory frameworks but \nalso to dynamically generate tailored audit paths, collect and validate evidence, and output both \ncontrol-level and executive-level risk evaluations.  \n  \nThis work is part of a joint thesis collaboration. Harsh will focus on the adversarial and technical \nvulnerability dimension of LLM security. Specifically, his contributions will include formalizing \nspecifications from the OWASP LLM Top 10, implementing both static and dynamic analysis tools \nfor detecting prompt injection and manipulation vulnerabilities, and validating those tools against \nbenchmark LLM systems in real-world conditions.  \n  \nMy role in the thesis will focus on the compliance alignment and risk assessment side. I will \ndevelop and refine a research -backed, LLM -interactive assessment framework that maps \ncompliance standards to real-world LLM deployment contexts. This includes ref ining a baseline \ncontrol checklist, engineering dialogue flows to extract organizational attributes, and integrating \nautomated control selection logic. I will also conduct literature-informed analysis on the limitations \nof current compliance guidance for AI systems and propose evaluation metrics for assessing audit \ncoverage, control relevance, and reporting clarity.   \nB.     Identification of Problem or Opportunity  \nWhile LLMs are increasingly embedded in high -impact business operations, most organizations \nlack reliable methods to assess their compliance posture against evolving AI governance \nstandards. Frameworks such as the NIST AI Risk Management Framework and ISO/ IEC 42001 \nprovide high-level guidance but fall short on offering concrete, testable criteria for LLM -specific \ndeployments [1], [2]. Manual audits, meanwhile, are resource -intensive, inconsistent, and not \neasily scalable [3], [4]. This creates a significant operational blind spot, where LLM-driven systems \nare deployed without clear risk visibility or assurance of regulatory alignment. However, the \nadaptive reasoning and conversational capabilities of LLMs themselves present a promising \nopportunity: enabling compliance assessments that are interactive, context -aware, and \ndynamically tailored to an organization\u2019s regulatory footprint [5], [6]. This research aims to \nformalize and test such a framework, addressing a growing need for scalable, standards-aligned \ncompliance tooling in enterprise AI environments.  \nC.      Stakeholders and Beneficiaries of Research  \n1. Academic researchers in AI governance and compliance will gain a replicable \nframework for operationalizing AI regulations, and empirical insights into AI risk \nevaluation methodologies [2], [6].  \n2. Enterprise compliance and risk management teams , who will benefit from \nautomated, context-aware audit tooling that aligns with evolving frameworks like \nNIST AI RMF and ISO/IEC 42001 [1], [3].  \n3. Regulatory bodies and third -party auditors, who can leverage standardized, \ntraceable reporting artifacts that enhance oversight and reduce ambiguity in AI \nsystem evaluations [4], [5].  \n4. LLM service providers and solution architects, seeking to embed transparent, \ntestable compliance checks into deployment workflows, strengthening trust with \nenterprise customers and regulators [7].  \nIII. Criteria  \n    A.     Level of Success  \n      1.     Minimum  \nThe research will produce a baseline compliance assessment framework that maps key \nrequirements from the NIST AI Risk Management Framework and ISO/IEC 42001 to structured, \nLLM-compatible audit questions. It will demonstrate the ability to programmatically capture \norganizational attributes through guided prompts, identify applicable controls, and generate an \ninitial compliance checklist. The system will output basic control mappings and support rationale, \nenabling structured evaluation of governance coverage  across different deployment contexts \n[1],[2].       \n      2.     Expected  \nAt this level, the framework will include a functioning LLM-driven interface that dynamically adapts \naudit flows based on organizational inputs such as industry, model usage, and regulatory scope. \nIt will implement rule -based logic to generate control appl icability scores, automate evidence \ncollection guidance, and produce formalized compliance reports. The output will be benchmarked \nacross at least three distinct LLM deployment scenarios, with evaluation metrics covering report \ncompleteness, control relevance, and usability for compliance analysts [3], [4].  \n      3.     Aspirational  \nTo reach its full potential, the research will incorporate intelligent control reasoning using LLM -\ngenerated justifications for control applicability, integrate a scoring engine for risk severity and \nmitigation priority, and support continuous compliance tracking over time. It will propose a method \nfor integrating these outputs with organizational governance dashboards or CI/CD tooling, \nenabling near real -time compliance visibility. The final contribution will be a publishable \nmethodology demonstrating how LLMs can drive practical, scalable alignment with AI governance \nstandards [5], [6].  \n    B.     Targets  \n1. Design and implement a structured compliance mapping system that aligns core functions \nfrom NIST AI RMF and clauses from ISO/IEC 42001 with context -aware audit questions \nfor LLM deployments.  \n2. Develop an adaptive logic engine that selects and ranks applicable compliance controls \nbased on organizational inputs across at least three industry scenarios.  \n3. Generate complete compliance reports, achieving maximum coverage of relevant controls \nper scenario, with stakeholder-rated clarity and usability scores above 70% in qualitative \nfeedback.  \n  \nIV.   Proposed Thesis  \nPrior work has demonstrated the growing relevance of AI-powered compliance tooling in \napplication and infrastructure security, but many efforts stop short of delivering dynamic, LLM -\nspecific, and standards-aligned audit workflows. For example, Gajbhiye et al. explored the use of \nAI technologies for regulator y compliance automation in application security, highlighting \ncapabilities such as policy enforcement, audit trail management, and real-time monitoring, but did \nnot address how these tools might adapt to LLM -specific controls or deployment contexts [1]. \nSimilarly, Kothandapani examined AI-driven compliance in the financial sector and introduced the \nuse of LLMs for text-to-code translation and regulatory interpretation yet did not explore evidence \nvalidation or interactive audit sequencing for LLM-based systems [2].  \nAli et al. proposed an automated compliance framework for critical infrastructure security using \nAI, but the design remained limited to static compliance mappings, with little integration of dynamic \ncontrol selection or regulatory tailoring based on busine ss attributes [3]. Meanwhile, Shetty\u2019s \nperspective on AI and security reinforced the need for organizations to align their AI systems with \nframeworks like NIST AI RMF yet emphasized the challenges of adapting these principles to \noperational deployments without offering automation strategies [4].  \nIn contrast to these efforts, this thesis introduces a compliance framework that actively engages \nwith an organization\u2019s operational profile to deliver dynamic, control-relevant audit flows for LLM \ndeployments. It leverages the reasoning capabilities of LL Ms to translate high-level governance \nmandates into contextualized audit paths, supported by structured evidence collection and \nexplainable control selection. Specifically, this research:  \n1. Operationalizes ISO/IEC 42001 and NIST AI RMF using logic -based mapping to \ndrive tailored control applicability.  \n2. Automates risk assessment interviews via interactive, LLM-based dialogue agents \nthat adjust based on regulatory scope, sector, and data use.  \n3. Produces structured, standards -aligned reports with mapped controls, rationale, \nand evidence pointers to support internal audit and external review.  \nA.     Research Questions:  \nThis thesis investigates whether Large Language Models can serve as an effective interface for \nautomating and contextualizing AI compliance assessments. The following research questions \nguide the study:  \nRQ1: How can compliance requirements from NIST AI RMF and ISO/IEC 42001 be \noperationalized into structured, LLM-driven audit logic?  \nRQ2: To what extent can an interactive LLM interface accurately identify applicable compliance \ncontrols based on organizational inputs (e.g., domain, deployment scope, regulatory \nobligations)?  \nRQ3: How effective is the proposed framework in generating complete, standards -aligned audit \nreports with minimal manual intervention?  \nRQ4: What metrics can be used to evaluate the usefulness, accuracy, and coverage of AI -\ngenerated compliance assessments in comparison to manual audit processes?  \nB.    Initial Baseline Checklist  \nBelow is a preliminary version of the baseline compliance checklist. Each entry maps a \ngovernance requirement to a control objective, provides an example of how it applies to LLM \nsystems, and outlines expected evidence types.  \nStandard Function/ \nClause \nLLM-Relevant Control \nObjective \nExample \nImplementation \nEvidence Type \nNIST AI  \nRMF  \nGovern  \n(GOV.1-\nGOV.3)  \nDefine and document AI \nsystem use, purpose, and  \nrisk posture  \nMaintain documented LLM \nuse cases and approval  \nworkflows  \n  Governance  \npolicy, use case \nregistry  \nNIST AI  \nRMF  \nMap  \n(MAP.1)  \nIdentify context, users, \ndata, and impacts  Classify training inputs \nand intended user \naudiences  \nData inventory, \ncontext mapping \nreport  \nISO/IEC  \n42001  \nClause  \n8.2.1  \nControl access to AI \nmodels and interfaces  Implement RBAC for LLM  \nAPIs and fine-tuning \nworkflows  \nIAM logs, access \ncontrol matrix  \nISO/IEC  \n42001  \nClause  \n6.1.2  \nRisk assessment procedure \nfor AI-specific deployment    Evaluate misuse  \nscenarios like prompt \ninjection or output abuse  \nThreat model, risk \nregister  \nNIST AI  \nRMF  \nManage \n(MAN.4)  \nIncident response for  \nAI-generated harms  Escalation plan for harmful \ncompletions or output \nanomalies  \n  IR procedure,  \nincident log \nsamples  \n  \n V.  Project Plan  \n   A.  Detailed Milestones for Key Deliverables  \nQuarter  Weeks  Tasks and Milestones  \nAutumn 2025  Weeks 1\u20134  \nLiterature review on NIST AI RMF, ISO/IEC 42001; study \nregulatory gaps in LLM systems  \n  Weeks 5\u20138  \nDevelop initial baseline compliance checklist; map standards to  \nLLM-specific controls  \n  Weeks 9\u201312  \nDesign an interactive audit engine; define dialogue flow and control \napplicability logic  \nWinter 2026  Weeks 1\u20134  \nBuild a dynamic audit generator and control selection engine using \nsimulated deployment scenarios \n  Weeks 5\u20138  \nImplement a structured evidence collection module; define \nvalidation and scoring rules  \n  Weeks 9\u201312  \nGenerate formalized compliance reports and evaluate framework \non 3 deployment cases; gather feedback and finalize write-up \n \n  B.   Work to be Completed  \nThe project will follow an iterative, research-driven development model. The process begins with \na grounded literature review to inform architecture and compliance mappings. Design will follow \na modular approach, enabling early testing of each component (e.g., audit generation, control \nlogic) before full system integration. Evaluation will involve scenario-based walkthroughs using \nsimulated LLM deployment cases with synthetic or placeholder evidence . Metrics will include \ncontrol coverage, reporting accuracy, and stakeholder usability feedback. A mix of qualitative and \nquantitative methods will be used to assess outcomes.  \n  \nVI.  Constraints, Risks, and Resources  \n    A.     Key Constraints  \n1. Scope Limitation: This thesis will focus on a representative subset of ISO/IEC \n42001 controls and selected subcategories from all four NIST AI RMF Functions \n(Govern, Map, Measure, Manage). The framework is designed to be extensible \nto the full standards, but validation is limited to a focused set of controls to ensure \nfeasibility within the thesis timeline. \n2. Access to Representative Use Cases : The effectiveness of the compliance \nframework depends on applying it to varied realistic LLM deployment scenarios. \nLack of access to real -world enterprise configurations may limit generalizability. \nSynthetic or placeholder evidence will be used rather than real enterprise data. \n3. Time and Scope : As the project involves both system design and research \nvalidation, time must be carefully managed to ensure completion without \noverextending into feature-heavy implementations.  \n4. Evolving Standards : Both NIST AI RMF and ISO/IEC 42001 are subject to \nrevisions and evolving interpretations. The framework will need to work with \ncurrent documentation, with clear assumptions about versioning.  \n5. Domain Familiarity: While having experience in cybersecurity and compliance, \nhands-on familiarity with LLM development and orchestration is limited. Additional \ntime and study may be required to fully prototype complex LLM -based dialogue \nflows and logic systems.  \n    B.     Resources Needed for Success  \n1. Technical Stack: Open-source LLMs (e.g., Hugging Face Transformers), Python, \nLangChain or similar orchestration tools, and vector databases for storing audit \nsessions.  \n2. Framework Documents: Official documentation and mappings for NIST AI RMF \nand ISO/IEC 42001; supporting literature from industry and academic sources.  \n \n3. Test Environments : Simulated or anonymized enterprise LLM deployment \nconfigurations to test control applicability and audit output quality.  \n      C.      Anticipated Risks  \n1. Framework Complexity: The logic required to dynamically select and adapt \ncompliance controls may grow complex, potentially leading to scalability issues \nor unexpected edge cases.  \n2. LLM Output Variability: As the dialogue engine relies on LLMs, non-\ndeterministic outputs or hallucinations may introduce inconsistency in audit paths \nor responses unless carefully constrained and validated.  \n \nVII.  Research Methods & Design  \nThis thesis follows a design science research methodology, focusing on the development and \nevaluation of a novel compliance framework for LLM systems. The objective is not only to build a \nfunctional tool, but to investigate its effectiveness, reproducibility, and alignment with recognized \nAI governance frameworks.  \nA. Framework Design Approach  \nThe framework will be developed as a modular system, consisting of:  \n\u25cf An LLM-driven dialogue interface to capture organizational context.  \n\u25cf A logic engine that maps inputs to applicable controls from NIST AI RMF and ISO/IEC \n42001.  \n\u25cf A questionnaire generator that produces a custom audit path.  \n\u25cf A reporting module that outputs both control-level and executive-level summaries.  \nDesign decisions will be informed by:  \n\u25cf Prior literature on AI compliance tooling [1][2].  \n\u25cf Structure and semantics of the chosen standards.  \n\u25cf Realistic organizational deployment scenarios collected via case study templates.  \n  \nB. Evaluation Methodology  \nThe framework will be evaluated using scenario-based testing and expert review. This includes: \n\u2022 Applying the system to three simulated LLM deployment cases: \n1. Financial Services Assistant: focusing on accountability, access control, and \nrisk assessment. \n2. Customer Service Chatbot: focusing on incident handling, transparency, and \nmonitoring outputs. \n3. University Knowledge Assistant: focusing on data governance, fairness, and \ncontinuous monitoring. \n\u2022 Measuring output completeness: whether the system surfaces maximum relevant \ncontrols for each scenario. \n\u2022 Assessing accuracy: whether controls selected are appropriate for the scenario context \n(e.g., financial risk controls appear in the financial assistant use case). \n\u2022 Evaluating clarity: reviewers rate report readability, structure, and actionability on a 5-\npoint scale. \n\u2022 Usability review: qualitative feedback from compliance professionals or graduate peers \non how well the reports support decision-making across different scenarios. \n  \nC. Validation Criteria  \nTo assess the framework\u2019s effectiveness, the following criteria will be used:  \n\u25cf Control Coverage: Percentage of relevant controls accurately surfaced based on input \nparameters.  \n\u25cf Report Quality: Expert -rated usefulness of compliance reports using a structured rubric \n(e.g., completeness, actionability).  \n\u25cf Audit Path Efficiency: Reduction in total controls shown compared to full checklist, without \ncompromising relevance.  \n\u25cf Repeatability: Whether similar inputs produce consistent, traceable outputs across \nmultiple sessions.  \n \nVIII.  Appendix  \nInitial Baseline Compliance Checklist (Full Table)  \n\u25cf Expanded table version of the five selected controls from NIST AI RMF and ISO/IEC 42001  \n\u25cf Includes control ID, standard source, objective, LLM -specific application, and evidence \ntype  \n  \nSample Audit Output  \n\u25cf Mockup of a system-generated audit report  \n\u25cf Includes: relevant controls selected, rationale, evidence prompts, and summary \ndashboard for stakeholders  \n  \nEvaluation Rubric (Draft)  \n\u25cf Rubric used to score coverage, relevance, and usability of the compliance framework  \n\u25cf Criteria for completeness, control accuracy, and reporting clarity  \nIX.  References  \n[1] B. Gajbhiye, S. Khan, and O. Goel, \"Regulatory Compliance in Application Security Using \nAI Compliance Tools,\" International Research Journal of Modernization in Engineering, \nTechnology and Science , vol. 6, no. 8, pp. 2583 \u20132585, Aug. 2024.  [Online]. Available: \nhttps://www.irjmets.com  \n[2] H. P . Kothandapani, \"AI-Driven Regulatory Compliance: Transforming Financial Oversight \nthrough Large Language Models and Automation,\" Emerging Science Research, vol.3, Jan. 2025. \n[Online]. Available: https://www.researchgate.net/publication/388231248  \n \n[3] S. M. Ali et al., \"An Automated Compliance Framework for Critical Infrastructure Security \nThrough Artificial Intelligence,\" IEEE Access, vol. 11, pp. 74459\u201374472, 2023, \n doi: 10.1109/ACCESS.2023.3275907.  \n \n[4] P. Shetty, \"AI and Security: From an Information Security and Risk Manager Standpoint,\" \nIEEE Access, vol. 12, pp. 77468\u201377480, Jun. 2024, doi: 10.1109/ACCESS.2024.3408144.  \n[5] S. Dambe, \"The Role of Artificial Intelligence in Enhancing Cybersecurity and Internal \nAudit,\" in 2023 3rd Int. Conf. on Advancement in Electronics & Communication Engineering \n(AECE), IEEE, pp. 85\u201390, doi: 10.1109/AECE59614.2023.10428353.  \n[6] A. Mohammed, \"AI in Cybersecurity: Enhancing Audits and Compliance Automation,\" \nInnovative Computer Science Journal , vol. 7, no. 1, pp. 1 \u20134, 2023. [Online]. Available:  \nhttps://innovatesci-publishers.com  \n[7] K. Doshi, \"Revolutionizing Compliance with Automation and AI,\" Int. Journal of Science, \nEngineering and Technology, vol. 11, no. 5, pp. 120\u2013124, Sep. 2023,  \ndoi: 10.61463/ijset.vol.11.issue5.567.  \n   \n",
  "checklist": [
    {
      "Framework / Control ID": "1. Governance & Accountability",
      "Control Name / Clause Summary": NaN,
      "LLM-Relevant Control Objective": NaN,
      "Example Implementation / Safeguard": NaN,
      "Evidence Type": NaN
    },
    {
      "Framework / Control ID": "NIST GOV 1.2",
      "Control Name / Clause Summary": "Policies Processes & Procedures",
      "LLM-Relevant Control Objective": "Define and document the organization's official policy for Generative AI/LLM use.",
      "Example Implementation / Safeguard": "Maintain a centralized \"AI Acceptable Use Policy\" (AUP) that explicitly addresses LLMs.",
      "Evidence Type": "AI Governance Policy, AUP Document"
    },
    {
      "Framework / Control ID": "NIST GOV 2.2",
      "Control Name / Clause Summary": "Roles & Responsibilities",
      "LLM-Relevant Control Objective": "Designate a \"Model Owner\" and \"AI Risk Officer\" with authority to stop unsafe deployments.",
      "Example Implementation / Safeguard": "Document specific AI roles in job descriptions or a RACI matrix.",
      "Evidence Type": "RACI Chart, Job Descriptions"
    },
    {
      "Framework / Control ID": "NIST GOV 5.2",
      "Control Name / Clause Summary": "Workforce Competence",
      "LLM-Relevant Control Objective": "Ensure employees using LLMs (devs & prompt engineers) are trained on AI risks.",
      "Example Implementation / Safeguard": "Mandatory \"AI Security Awareness\" training module for all staff interacting with the model.",
      "Evidence Type": "Training Logs, Certifications"
    },
    {
      "Framework / Control ID": "ISO 42001 A.2.2",
      "Control Name / Clause Summary": "Policy Alignment",
      "LLM-Relevant Control Objective": "Ensure AI objectives align with broader business strategy (business value vs. risk).",
      "Example Implementation / Safeguard": "Define clear KPIs for the LLM project that map to business goals.",
      "Evidence Type": "Project Charter, Strategic Plan"
    },
    {
      "Framework / Control ID": "ISO 42001 A.3.2",
      "Control Name / Clause Summary": "Reporting Concerns",
      "LLM-Relevant Control Objective": "Establish a whistleblower or feedback channel for employees to report AI anomalies.",
      "Example Implementation / Safeguard": "Create an anonymous web form or email alias for reporting \"AI Hallucinations/Harms.\"",
      "Evidence Type": "Internal Portal Link, Whistleblower Process"
    },
    {
      "Framework / Control ID": "2. Data Integrity & Provenance",
      "Control Name / Clause Summary": NaN,
      "LLM-Relevant Control Objective": NaN,
      "Example Implementation / Safeguard": NaN,
      "Evidence Type": NaN
    },
    {
      "Framework / Control ID": "ISO 42001 A.7.1",
      "Control Name / Clause Summary": "Data Governance",
      "LLM-Relevant Control Objective": "Define ownership and classification for data entered into or used to fine-tune the LLM.",
      "Example Implementation / Safeguard": "Tag all training data with classification levels (e.g., Public, Confidential, PII).",
      "Evidence Type": "Data Classification Policy, Tagging Logs"
    },
    {
      "Framework / Control ID": "ISO 42001 A.7.2",
      "Control Name / Clause Summary": "Data Acquisition",
      "LLM-Relevant Control Objective": "Verify legal rights (IP/Copyright) for all training data and RAG sources.",
      "Example Implementation / Safeguard": "Legal review of all scraped data or third-party datasets to ensure commercial use rights.",
      "Evidence Type": "License Agreements, Terms of Use"
    },
    {
      "Framework / Control ID": "ISO 42001 A.7.3",
      "Control Name / Clause Summary": "Data Quality",
      "LLM-Relevant Control Objective": "Assess training data for poisoning, bias, or incompleteness before use.",
      "Example Implementation / Safeguard": "Run statistical checks on datasets to identify class imbalances or corrupted files.",
      "Evidence Type": "Data Quality Report, Sampling Logs"
    },
    {
      "Framework / Control ID": "ISO 42001 A.7.4",
      "Control Name / Clause Summary": "Data Provenance",
      "LLM-Relevant Control Objective": "Maintain a \"Data Bill of Materials\" tracking the lineage of all datasets used.",
      "Example Implementation / Safeguard": "Create a document listing the source, version, and modifications of every dataset.",
      "Evidence Type": "Data Inventory, Lineage Diagram"
    },
    {
      "Framework / Control ID": "NIST MAP 2.2",
      "Control Name / Clause Summary": "Privacy & Confidentiality",
      "LLM-Relevant Control Objective": "Check for PII leakage risks in training data and prompt outputs.",
      "Example Implementation / Safeguard": "Scrub PII from training sets; implement PII detection filters on model inputs/outputs.",
      "Evidence Type": "DLP Logs, Privacy Impact Assessment (PIA)"
    },
    {
      "Framework / Control ID": "3. Risk Assessment & Supply Chain",
      "Control Name / Clause Summary": NaN,
      "LLM-Relevant Control Objective": NaN,
      "Example Implementation / Safeguard": NaN,
      "Evidence Type": NaN
    },
    {
      "Framework / Control ID": "NIST MAP 1.1",
      "Control Name / Clause Summary": "Context Establishment",
      "LLM-Relevant Control Objective": "Explicitly define the intended purpose and prohibited uses of the model.",
      "Example Implementation / Safeguard": "Document \"Out of Scope\" use cases (e.g., \"Medical Advice is prohibited\").",
      "Evidence Type": "Concept of Operations (ConOps)"
    },
    {
      "Framework / Control ID": "NIST MAP 3.1",
      "Control Name / Clause Summary": "Risk Likelihood/Impact",
      "LLM-Relevant Control Objective": "Evaluate specific LLM risks: hallucinations, jailbreaks, and harmful content.",
      "Example Implementation / Safeguard": "Create a dedicated \"AI Risk Register\" distinct from general IT risks.",
      "Evidence Type": "Risk Register (AI Specific)"
    },
    {
      "Framework / Control ID": "ISO 42001 A.5.1",
      "Control Name / Clause Summary": "Impact Assessment",
      "LLM-Relevant Control Objective": "Assess impact on individuals (fairness) and society (disinformation) before deployment.",
      "Example Implementation / Safeguard": "Conduct an Algorithmic Impact Assessment (AIA) for high-risk deployments.",
      "Evidence Type": "Algorithmic Impact Assessment (AIA)"
    },
    {
      "Framework / Control ID": "ISO 42001 Cl. 8.4",
      "Control Name / Clause Summary": "Supplier Relationships",
      "LLM-Relevant Control Objective": "Assess third-party LLM providers (e.g., OpenAI, Anthropic) for their own compliance.",
      "Example Implementation / Safeguard": "Review SOC2 Type II reports or security attestations from the model provider.",
      "Evidence Type": "Vendor Risk Assessment, SOC2 Report"
    },
    {
      "Framework / Control ID": "NIST MAP 4.1",
      "Control Name / Clause Summary": "Third-Party Dependencies",
      "LLM-Relevant Control Objective": "Map risks from external APIs, plugins, or vector databases.",
      "Example Implementation / Safeguard": "Diagram all data flows between the LLM and external tools (e.g., LangChain agents).",
      "Evidence Type": "System Architecture Diagram, Data Flow Map"
    },
    {
      "Framework / Control ID": "4. Technical Reliability & Security",
      "Control Name / Clause Summary": NaN,
      "LLM-Relevant Control Objective": NaN,
      "Example Implementation / Safeguard": NaN,
      "Evidence Type": NaN
    },
    {
      "Framework / Control ID": "NIST MEA 2.7",
      "Control Name / Clause Summary": "Adversarial Testing",
      "LLM-Relevant Control Objective": "Conduct \"Red Teaming\" for prompt injection, jailbreaking, and extraction attacks.",
      "Example Implementation / Safeguard": "Hire external red-teamers or use automated tools (e.g., GARAK) to probe the model.",
      "Evidence Type": "Pentest Report, Red Team Logs"
    },
    {
      "Framework / Control ID": "NIST MEA 2.2",
      "Control Name / Clause Summary": "Validity & Reliability",
      "LLM-Relevant Control Objective": "Measure hallucination rates and factual accuracy against a \"Golden Dataset.\"",
      "Example Implementation / Safeguard": "Test model outputs against a set of verified Q&A pairs; score utilizing ROUGE/BLEU.",
      "Evidence Type": "Evaluation Metrics Report"
    },
    {
      "Framework / Control ID": "ISO 42001 A.6.3",
      "Control Name / Clause Summary": "Secure Development",
      "LLM-Relevant Control Objective": "Use secure coding practices for the application layer wrapping the LLM.",
      "Example Implementation / Safeguard": "Sanitize all user inputs before passing them to the LLM (prevent prompt injection).",
      "Evidence Type": "SAST/DAST Scan Reports"
    },
    {
      "Framework / Control ID": "ISO 42001 A.6.4",
      "Control Name / Clause Summary": "Verification & Validation",
      "LLM-Relevant Control Objective": "Formally sign off that the model meets requirements before production release.",
      "Example Implementation / Safeguard": "Require \"Go/No-Go\" signature from the AI Risk Officer before deployment.",
      "Evidence Type": "V&V Sign-off, Acceptance Test Report"
    },
    {
      "Framework / Control ID": "ISO 27001 A.5.15",
      "Control Name / Clause Summary": "Access Control",
      "LLM-Relevant Control Objective": "Restrict access to model weights, API keys, and fine-tuning datasets (RBAC).",
      "Example Implementation / Safeguard": "Use distinct API keys for dev/prod; enforce MFA for access to the model registry.",
      "Evidence Type": "IAM Logs, Key Management Logs"
    },
    {
      "Framework / Control ID": "5. Transparency & Documentation",
      "Control Name / Clause Summary": NaN,
      "LLM-Relevant Control Objective": NaN,
      "Example Implementation / Safeguard": NaN,
      "Evidence Type": NaN
    },
    {
      "Framework / Control ID": "ISO 42001 A.8.2",
      "Control Name / Clause Summary": "System Documentation",
      "LLM-Relevant Control Objective": "Maintain a \"Model Card\" detailing parameters, limitations, and training data cutoff.",
      "Example Implementation / Safeguard": "Publish a Model Card accessible to all internal developers/users.",
      "Evidence Type": "Model Card, System Card"
    },
    {
      "Framework / Control ID": "ISO 42001 A.8.1",
      "Control Name / Clause Summary": "User Notification",
      "LLM-Relevant Control Objective": "Clearly label AI-generated content to end-users (watermarking or UI disclosures).",
      "Example Implementation / Safeguard": "Add a disclaimer in the Chat UI: \"AI generated content. Check for errors.\"",
      "Evidence Type": "UI Screenshots, User Manual"
    },
    {
      "Framework / Control ID": "NIST MEA 2.11",
      "Control Name / Clause Summary": "Explainability",
      "LLM-Relevant Control Objective": "Document the logic of the system (or retrieval sources for RAG) to justify outputs.",
      "Example Implementation / Safeguard": "Ensure the RAG system provides citations/links to source documents for every answer.",
      "Evidence Type": "Explainability Report, Source Citations"
    },
    {
      "Framework / Control ID": "ISO 42001 A.8.3",
      "Control Name / Clause Summary": "Incident Communication",
      "LLM-Relevant Control Objective": "Plan for notifying users/regulators if the AI causes harm or leaks data.",
      "Example Implementation / Safeguard": "Draft templates for breach notifications specific to AI failures.",
      "Evidence Type": "Communication Plan, Draft Notifications"
    },
    {
      "Framework / Control ID": "ISO 42001 A.10.2",
      "Control Name / Clause Summary": "Customer Agreements",
      "LLM-Relevant Control Objective": "Clearly define liability and responsibility for AI outputs in customer contracts.",
      "Example Implementation / Safeguard": "Update Terms of Service (ToS) to disclaim liability for hallucinations.",
      "Evidence Type": "Terms of Service (ToS), SLA"
    },
    {
      "Framework / Control ID": "6. Monitoring & Continuous Improvement",
      "Control Name / Clause Summary": NaN,
      "LLM-Relevant Control Objective": NaN,
      "Example Implementation / Safeguard": NaN,
      "Evidence Type": NaN
    },
    {
      "Framework / Control ID": "NIST MAN 4.1",
      "Control Name / Clause Summary": "Continuous Monitoring",
      "LLM-Relevant Control Objective": "Real-time monitoring for drift, toxicity, and refusal rates.",
      "Example Implementation / Safeguard": "Implement a dashboard tracking user downvotes and \"refusal\" flags.",
      "Evidence Type": "Monitoring Dashboard, Alert Logs"
    },
    {
      "Framework / Control ID": "NIST MAN 3.2",
      "Control Name / Clause Summary": "Incident Response",
      "LLM-Relevant Control Objective": "Playbook for handling \"Prompt Injection\" or \"Mass Hallucination\" events.",
      "Example Implementation / Safeguard": "Create specific runbooks for \"disconnecting\" the model if it goes rogue.",
      "Evidence Type": "IR Playbook (AI Specific)"
    },
    {
      "Framework / Control ID": "ISO 42001 A.6.6",
      "Control Name / Clause Summary": "Re-evaluation (Drift)",
      "LLM-Relevant Control Objective": "Trigger re-training or re-validation if model performance drops below a threshold.",
      "Example Implementation / Safeguard": "Set automated alerts if model accuracy drops below 90% on the golden set.",
      "Evidence Type": "Retraining Logs, Drift Reports"
    },
    {
      "Framework / Control ID": "ISO 42001 A.6.7",
      "Control Name / Clause Summary": "Decommissioning",
      "LLM-Relevant Control Objective": "Securely delete model weights and vector embeddings when the system is retired.",
      "Example Implementation / Safeguard": "Process to wipe vector DBs and model checkpoints upon project termination.",
      "Evidence Type": "Data Destruction Certificate"
    },
    {
      "Framework / Control ID": "NIST  GOV 2.3 and MEA 2.6",
      "Control Name / Clause Summary": "Human Oversight",
      "LLM-Relevant Control Objective": "Implement \"Human-in-the-loop\" (HITL) for high-stakes decisions.",
      "Example Implementation / Safeguard": "Route low-confidence answers to a human reviewer before showing the user.",
      "Evidence Type": "Approval Workflows, HITL Logs"
    }
  ]
}